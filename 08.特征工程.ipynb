{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 3)\t30.0\n",
      "  (1, 2)\t1.0\n",
      "  (1, 3)\t35.0\n",
      "  (2, 1)\t1.0\n",
      "  (2, 3)\t33.0\n",
      "[[ 1.  0.  0. 30.]\n",
      " [ 0.  0.  1. 35.]\n",
      " [ 0.  1.  0. 33.]]\n",
      "{'city=beijing': 0, 'temperture': 3, 'city=shanghai': 2, 'city=guangzhou': 1}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "字典特征提取\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction import DictVectorizer      # 机器学习工具包下的特征提取模块中的字典特征提取模块\n",
    "# 使用DictVectorizer对使用字典存储的数据进行特征抽取和矢量化\n",
    "# DictVectorizer对非数字化的处理方式是，借助原特征的名称，组合成新的特征，并采用0/1的方式进行矢量化。\n",
    "\n",
    "dv = DictVectorizer()                                    # 初始化一个字典特征处理器 参数sparse=False 把坐标转成高维数组(默认为True)\n",
    "# 训练，转换并返回\n",
    "data = dv.fit_transform([{'city':'beijing','temperture':30},          \n",
    "                         {'city':'shanghai','temperture':35},\n",
    "                         {'city':'guangzhou','temperture':33}])      # 城市 温度\n",
    "# fit()方法的主要工作是获取特征信息和目标值信息，在这点上fit方法和模型训练时的fit方法就能够联系在一起了\n",
    "# 都是通过分析特征和目标值，提取有价值的信息，对于转换类来说是某些统计量，对于模型来说可能是特征的权值系数等。\n",
    "# fit_transform()方法对数据先fit训练拟合出模型,然后进行转换 相当于把一种数据转换为另一种矢量化数据\n",
    "\n",
    "print(data)                                                          # 坐标与坐标值\n",
    "# print(data.todense())                                                # 把坐标转成高维数组(矩阵类型)\n",
    "# print(data.toarray())                                                # .toarray()方法理解为：编码过程 把坐标转成高维数组 总特征数量 beijing 30 \n",
    "# print(dv.inverse_transform([[1,0,0,30]]))                            # .inverse_transform()方法理解为：解码过程 取出坐标对应数据\n",
    "# print(dv.transform([{'city':'beijing','temperture':30}]).toarray())  # 经过训练后的DictVectorizer()不需要再次训练直接使用transform()转换就可以提取\n",
    "# print(dv.get_feature_names())                                        # 提取特征标题\n",
    "# print(dv.vocabulary_)                                                # 获取词频 （特征列：对应特征标题所在下标索引位置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dislike', 'is', 'java', 'life', 'like', 'long', 'money', 'php', 'python', 'short', 'too']\n",
      "{'life': 3, 'is': 1, 'short': 9, 'like': 4, 'python': 8, 'too': 10, 'long': 5, 'dislike': 0, 'java': 2, 'php': 7, 'money': 6}\n",
      "[[0 1 0 1 1 0 0 0 1 1 0]\n",
      " [1 1 0 1 0 1 0 0 1 0 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 1 0 1 0 0 0 0]]\n",
      "[[0 1 0 1 1 0 0 0 1 1 0]\n",
      " [1 1 0 1 0 1 0 0 1 0 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 1 0 1 0 0 0 0]]\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "文本特征提取\n",
    "\"\"\"\n",
    "# 提取英文特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer    # 机器学习工具包下的特征提取模块下的文本模块中的文本特征提取模块\n",
    "\n",
    "data = [\"Life is short,i like Python\",\"Life is too long,i dislike Python\",'java php c++','i is like money']\n",
    "cv = CountVectorizer()            # 初始化一个文本特征处理器 \n",
    "# 参数 vocabulary=['is'] 构建关键词集提取想要的特征文本 stop_words=['too'] 设置停用词排除不想要的特征文本 lowercase=True 所有特征词都变为小写默认为True\n",
    "data = cv.fit_transform(data)     # 训练，转换并返回\n",
    "print(cv.get_feature_names())     # 提取特征标题\n",
    "print(cv.vocabulary_)             # 获取词频（特征列：对应特征标题所在下标位置）\n",
    "print(data.toarray())             # 把坐标转成高维数组(ndarray类型)\n",
    "print(data.todense())             # 把坐标转成高维数组(矩阵类型)\n",
    "print(data)                       # 坐标与坐标值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.23606798]]\n",
      "[[2.82842712]]\n"
     ]
    }
   ],
   "source": [
    "# 计算欧氏距离\n",
    "from sklearn.metrics.pairwise import euclidean_distances     # 欧氏距离模块(两点向量之间的距离) 距离越大样本的相似度越低\n",
    "\n",
    "dist1 = euclidean_distances(X=data[0],Y=data[1])             # 初始化一个欧氏距离处理器 计算第一个与第二个数据的距离                                            \n",
    "dist2 = euclidean_distances(X=data[1],Y=data[2])             # 计算第二个与第三个数据的距离\n",
    "print(dist1)\n",
    "print(dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我 学会 了 python ， java 和 php', 'python 包括 flask ， django 和 数据库', 'java 和 php 要求 熟练掌握 数据库')\n",
      "['django', 'flask', 'java', 'php', 'python', '数据库']\n",
      "[[0 0 1 1 1 0]\n",
      " [1 1 0 0 1 1]\n",
      " [0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 提取中文特征\n",
    "import jieba                                                                # 分词模块\n",
    "def cutword():\n",
    "    contetn1 = jieba.lcut(\"我学会了python，java和php\")\n",
    "#     print(contetn1)\n",
    "    contetn2 = jieba.lcut(\"python包括flask，django和数据库\")\n",
    "    contetn3 = jieba.lcut(\"java和php要求熟练掌握数据库\")\n",
    "    c1 = ' '.join(contetn1)                                                 # 将分词以空格隔开\n",
    "#     print(c1)\n",
    "    c2 = ' '.join(contetn2)\n",
    "    c3 = ' '.join(contetn3)\n",
    "    return c1,c2,c3\n",
    "print(cutword())\n",
    "\n",
    "def getFeature():\n",
    "    count = CountVectorizer(stop_words=['要求','学会','包括','熟练掌握'])    # 初始化一个文本特征提取器，并设置停用词\n",
    "    a1, a2, a3 = cutword()                                                 # 变量接收并调用分词的函数\n",
    "#     print(a1)\n",
    "#     print(a2)\n",
    "#     print(a3)\n",
    "    data = count.fit_transform([a1, a2, a3])                               # 训练，转换并返回\n",
    "    print(count.get_feature_names())                                       # 提取特征标题\n",
    "    print(data.toarray())                                                  # 把坐标转成高维数组(ndarray类型)\n",
    "getFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['django', 'flask', 'java', 'php', 'python', '包括', '学会', '数据库', '熟练掌握', '要求']\n",
      "[[0.         0.         0.45985353 0.45985353 0.45985353 0.\n",
      "  0.60465213 0.         0.         0.        ]\n",
      " [0.49047908 0.49047908 0.         0.         0.37302199 0.49047908\n",
      "  0.         0.37302199 0.         0.        ]\n",
      " [0.         0.         0.3935112  0.3935112  0.         0.\n",
      "  0.         0.3935112  0.51741994 0.51741994]]\n"
     ]
    }
   ],
   "source": [
    "# tf-idf文本特征提取\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer     # tf-idf（词频计算方法）文本特征提取模块\n",
    "    # TF意思是词频，IDF意思是逆文本频率指数\n",
    "    # 某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。\n",
    "    # TF-IDF倾向于过滤掉常见的词语，保留重要的词语。\n",
    "    # 某个词对文章的重要性越高，它的TF-IDF值就越大\n",
    "    \n",
    "def tfidfvec():\n",
    "    tfidf = TfidfVectorizer()                             # 初始化一个tf-idf文本特征提取器\n",
    "    b1, b2, b3 = cutword()                                # 变量接收并调用分词的函数\n",
    "    data = tfidf.fit_transform([b1, b2, b3])              # 训练，转换并返回\n",
    "    print(tfidf.get_feature_names())                      # 提取特征标题\n",
    "    print(data.toarray())                                 # 把坐标转成高维数组(ndarray类型)\n",
    "    print(tfidf.vocabulary_)                              # 获取词频（特征列：对应特征标题所在下标位置）\n",
    "tfidfvec()    \n",
    "\n",
    "# 词频（term frequency，tf）指的是某一个给定的词语在'该文件中出现的频率'。                       频率越高该词在文件中出现的次数越多\n",
    "# 逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量(不同词重要性的度量)。 度量越低该词在所有文件中出现的次数越多\n",
    "# 某一特定词语的idf，可以由'总文件数目除以包含该词语之文件的数目'，再将得到的商，取以10为底的对数得到\n",
    "# log是以n为底m的对数如log m9 n3= 次方2（3*3）  lg为以10为底m的对数如lg（10,000,000 / 1,000）=4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.7071067811865475\n",
      "  (0, 4)\t0.7071067811865475\n",
      "  (1, 0)\t0.5773502691896258\n",
      "  (1, 1)\t-0.5773502691896258\n",
      "  (1, 4)\t0.5773502691896258\n",
      "[[ 0.          0.          0.70710678  0.          0.70710678]\n",
      " [ 0.57735027 -0.57735027  0.          0.          0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "# 哈希算法矢量化文本特征提取\n",
    "from sklearn.feature_extraction.text import HashingVectorizer           # HashingVectorizer（哈希算法矢量化）文本特征提取\n",
    "# 哈希算法不可逆无法通过解码转换对应文本\n",
    "hashV = HashingVectorizer(n_features=5,stop_words=['is'])                # 参数 n_features=5 只提取5个特征 \n",
    "hash_V_data = hashV.fit_transform(['java is good','python is very good'])  \n",
    "print(hash_V_data)\n",
    "print(hash_V_data.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '我们', '中国', '人', '厉害']\n",
      "['我们', '中国', '人', '的', 'python', '厉害']\n",
      "['我们', '我们', '中国', '人', '厉害', '我们', '中国', '人', '的', 'python', '厉害']\n",
      "{'我们': 3, '中国': 2, '人': 2, '厉害': 2, '的': 1, 'python': 1}\n",
      "[('我们', 3), ('中国', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 文本特征词频统计\n",
    "from collections import Counter                           # 收集模块中的统计模块\n",
    "import jieba                                              # 分词模块\n",
    "\n",
    "word_list1 = jieba.lcut('我们我们中国人厉害')               # .lcut()分词方法  返回的是一个列表\n",
    "print(word_list1)\n",
    "\n",
    "word_list2 = jieba.lcut('我们中国人的python厉害')\n",
    "print(word_list2)\n",
    "\n",
    "word_list1.extend(word_list2)                             # 合并两个jieba\n",
    "print(word_list1)\n",
    "\n",
    "counter = Counter(word_list1)                             # 初始化一个统计器\n",
    "dictionary=dict(counter)                                  # 转换dict用于显示词频个数\n",
    "print(dictionary)                                         # 统计词频个数\n",
    "print(counter.most_common(2))                             # most_common(2)取出前2个最大的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
